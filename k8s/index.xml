<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on My New Hugo Site</title>
    <link>https://scylhy.github.io/k8s/</link>
    <description>Recent content in K8s on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://scylhy.github.io/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/persistentvoumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/persistentvoumes/</guid>
      <description>Persistent Volumes introduce PersistentVolume子系统为用户和管理员提供一套将系统底层存储细节抽象的api，涉及两类API资源:PersistentVolume和PersistentVolumeClaim.
PersistentVolume(pv)是由集群管理员提供的一种存储，它和node一样是一种集群资源。PVs是一种类似Volumes的存储插件，但是它具有独立于pod的生命周期。PV 的存储实现方式有NFS、iSCSI以及云提供商制定的存储系统。
PersistentVolumeClaim(PVC)是一个存储请求，它类似于pod，pod消耗node资源，而pvc消耗pv资源；Pod能够请求指定级别的cpu和memory资源，pvc可以请求制定大小和获取模式(支持once read/write 和many times read only)的pv。
用户一般会用PVC去申请不同规格的PV，这些不同规格的PV应该集群管理员，这样就为用户屏蔽了底层存储实现的细节，而对应的规格是由另一种资源SotrageClass定义的。
在搭建kafaka/zookeeper集群时，stateful set需要动态绑定pv,一般动态绑定pv是需要云服务提供商的服务，本地需要另做处理。
lifecycle of a lolume and claim Provisioning阶段：
PVs由两种管理方式，static和dynamic。
static：集群管理员会创建大量的PV,静态方式会将存储细节暴露给集群用户，方便用户通过集群api使用存储。其实，觉得这更适合k8s的学习、开发者用。
dynamic: 当没有静态的PVs匹配用户的PVC时，集群就会尝试动态的为PVC提供一个数据卷。这种动态分配基于StorageClasses:PVC必须制定一个strage class,管理员必须创建PV时，必须配置storage class。在动态分配数据卷下，PVC通过storage class和数据卷联系。
Binding:
用户创建动态方式的PVC后，该PVC指定了所需的access mode。master上的控制循环会监视到新的PVC,如果有匹配的PV，就会绑定他们。一个PV如果被分配给一个PVC，控制循环将始终绑定该PV\PVC。一般用户会得到比他们申请规格大的数据卷。PV/PVC的绑定时排他的，这种绑定关系时一一映射的。
Using:
Pods使用claims获取存储,集群通过claims找到PVC/PV绑定，为pod挂在PV数据卷。数据卷支持多种访问模式，用户可以使用claims在pods上指定访问模式。
Reclaiming
当用户使用完他们的数据卷后，便可删除PVC对象，k8s API允许回收该数据卷。PersistentVolume的声明告诉集群在释放claim后应该怎么做。目前数据卷可以被Retained、Recycled和Deleted。
Retain（保留）:
Retain回收策略允许手动回收数据卷。当PVC被删除后，PV仍然存在，在pod看来，数据卷似乎被释放了，但该PV对于对于其他PVC仍是不可用的，因为之前使用者的数据还在数据卷里。管理员可以通过如下步骤手动回收该数据卷： 1. 删除PV,对于外部基础设施提供的相关存储在PV删除后，仍然存在。 2. 手动清理关联的存储上的数据。 3. 手动删除关联的存储，如果想重新使用该存储，可以创建一个指定该存储的PV。
Delete:
Delete回收策略会同时删除PV及对应的存储。动态配置的卷集成StorageClass的回收策略，其默认Delete，管理原应该为用户的期望来配置StorageClass，否则只能在卷创建后进行编辑和修补。
Recycle:
如果底层的存储卷插件支持，那Recycle回收策略会在该卷上执行基本的擦洗（rm -rf /thevolume/*)使其可被新的PVC使用。 然而，管理员也可以配置一个自定义的回收pod模版通过k8s控制管理命令擦洗卷。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/statefulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/statefulset/</guid>
      <description>Stateful Set 在k8s上部署有状态的应用是通过StatefulSets实现的。这里我们尝试部署官网的Stateful Set demo，为kfk/zk的部署做前期实验。
像Deployment一样，StatefulSet也是基于容器特定的标识来管理Pod的，不一样的是StatefulSet会为每个pod保持一个标识。StatefulSet下创建的pod具有相同的配置，但每个pod都有一个持久的标志，正是该标志可以保证pod部署和scale的顺序。
StatefulSets是用来解决有状态的应用的，比如： 1. 稳定的网络标识 2. 稳定的持久化存储 3. 有序部署、有序扩容 4. 有序收缩、有序删除
StatefulSets的使用限制： 1. StatefulSet在1.9之前是beta资源，在1.5之前是不可用的 2. Pod的存储必须由PersistentVolume Provisioner根据请求的storage class配置，或者由管理员预先配置。 3. 删除或者收缩不会删除与其相关的数据卷，其目的是保证数据安全，一般是比自动清除数据要可靠些。 4. 为了Pods的网络表识，StatefulSets要求创建Headless Service。 5. StatefulSet被删除，其不保证中止pods,为了有序平滑的中止pods,应该收缩StatefulSet到0，最后删除StatefulSet。 6. 在使用默认Pod管理策略（orderedReady)进行滚动升级是，可能会进入异常状态，这需要手动干预修复。
Components:
StatefulSet配置需要如下几个组件： 1. Headless Service，用来控制网络域名。 2. spec要制定副本数，以启动对应数量的pods。 3. volumeClaimTemplates,使用供应商提供的PV来提供稳定存储。
Pod Identity:
StatefulSet中的每个pod拥有一个独特的表识，包含有序索引、稳定的网络表识，稳定存储。
ordinal index：n个副本，索引则从0到n-1。 stable network id:pod的主机名为\$(statefulset name)-\$(ordinal),StatefulSet的域名为\$(service name).\$(namespace).svc.cluster.local。（cluster.local为集群域名），在pod创建后，会匹配一个DNS子域名：\$(podname).\$(service name)。
Stable storage:
k8s为每个VolumeClaimTemplate创建一个pv,pod会收到制定StorageClass和规格的pv,如果StorageClass没有指定，会使用默认的StorageClass。当pod被调度到node上，其volumeMounts会挂在pvc指定的pv。在StatefulSet/pod被删除后，其关联的pv不会自动删除，需要用户手动删除。
Pod Name Label:
StatefulSet控制器创建一个Pod，就会为其添加 statefulset.kubernetes.io/pod-name 标签，可以通过该标签在指定的pod上附着Service。
创建有状态的应用：
实践，在自己搭建的集群上，因为没有pv provisioner,所以我们需要使用本地的存储,在volumeClaimTemplates上使用自定义的storageClass,provisioner要设置为 kubernetes.io/no-provisioner;创建指定storageClass的pv;创建有状态的应用，通过storageClass使用对应的pv。
storageClass:
# Only create this for K8s 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/fabric.kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/fabric.kafka/</guid>
      <description>k8s部署fabric-kafka 在k8s上部署fabric witch kafka consensus，相比之前部署fabric with solo consensus的区别就在于多了一个zk/kfk集群，在我们部署好zk/kfk后，对fabric.solo简单修改即可应用。
回顾一下，fabric.solo上解决了哪些问题： 1. docker dns，即在宿主机上启动链码容器，并保持链码容器和k8s pod的内通信问题；另外还有个改进，就是用k8s启动链码容器，这个可以参考IBM官方部署方案。 2. kubeadm构建k8s集群，这里遇到了，集群重启(kubeadm reset-&amp;gt;kubeadm init)造成cni网络插件异常的情况。 3. 在单节点集群（只有master）上，k8s为master安全考虑，禁止在master node上调度节点，这只需要取消master taint即可。
在单节点k8s集群上，成功部署了fabric.solo；紧接着了，在k8s上又部署成功了zk/kfk集群。此时，部署fabric with kafka consensus的条件我们都具备了，那是否可部署成功了呢。 事实上，流程已经打通了，但没想到fabrci with kfk，需要太多的pod，这就导致，我那台2c4g的机器，cpu满载了，以至于无法操作终端。
（红线部分是当时的cpu满载情况）
所以，还有个工作要做，单节点集群无法满足fabric with kfk的性能要求，我们需要对k8s扩容，这里我又开了一台主机，打算搭建两节点的集群。这样后续，通过调度手段，将pod分布到两台机器上，就可以满足fabric with kfk的性能要求了。
在部署两节点的k8s集群上，又遇到了一些问题，我用官方的kubeadm教程方式搭建，在新机器上，安装的kube都是1.14.2的，索性为了统一，也将旧机器上的kube更换成了1.14.2。看似没问题，但是在启动集群时发现，拉取不到k8s.gcr.io下的镜像，配置代理后，仍是无法获取到k8s.gcr.io下镜像，奇怪的是，我那台旧机器上，居然有1.14.2的镜像，我也没使用过其他手段（比如大tag的方式）获取最新镜像呀。之前的版本都可以通过获取别人存的镜像，再打个tag就好，但是1.14.2可能太新，愣是没找到私人存储的镜像，所以当天工作大多是用来找镜像了。还好，最后找到aws为国内提供k8s.gcr.io的镜像站点gcr.azk8s.cn，通过该站点可以获取到最新的1.14.2镜像，之后再打tag就好。获取到镜像好，很快就部署好了双节点集群。
同样要注意的是，取消掉master node的taints(node-role.kubernetes.io/master),以及kubeadm reset之后的cni插件无法工作的处理。
这样，我们的集群有了两个可以部署pod节点，我们就可以通过affinity或者taints进行一个高级调度，将pod分散到两个node上，我这里采用了较为简单的方式，对node搭上标签，然后通过NodeSelector将pod调度到指定node上。我的安排是，zk/kfk在一个节点上，fabric那一套在一个节点上。
具体过程不在叙述，看一下搭建的集群状况。
root@hw2:~# kubectl version --short Client Version: v1.14.2 Server Version: v1.14.2 root@hw2:~# root@hw2:~# kubectl get pods -nkube-system -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-fb8b8dccf-lgr8p 1/1 Running 0 4h22m 10.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/ibm-kubernetes%E5%BE%AE%E8%AE%B2%E5%A0%82/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/ibm-kubernetes%E5%BE%AE%E8%AE%B2%E5%A0%82/</guid>
      <description>label
kubernetes中部署应用的过程
CRD,自定义资源
未来发展方向，出个stability releases，以及和生态的发展CNCF</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/k8s1.14.2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/k8s1.14.2/</guid>
      <description>kubeadm token create &amp;ndash;print-join-command</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/kfk-zk1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/kfk-zk1/</guid>
      <description>在k8s上搭建kfk-zk集群(1) kfk/zk属于有状态的应用，所以应当用StatefulSet来部署应用，但StatefulSet要比无状态的Deployment要复杂，主要表现在增加了：
 stable stroage，并且需要用StorageClass、PV 有状态应用的网络问题，需要Headless Service,并且每个pod是有序编号的，其DNS就是name-1.headlessService  这里，我们采用官方部署zk的教程,该教程有个前提（新手一般想打它） &amp;gt; This tutorial assumes that you have configured your cluster to dynamically provision PersistentVolumes. If your cluster is not configured to do so, you will have to manually provision three 20 GiB volumes before starting this tutorial.
这就给我们带来了一点麻烦,主要是创建PV以及如何让StatefulSet能够绑定我们创建的PV。那么，我们的工作就是创建PV，以及如何让StatefulSet绑定它们。
StatefulSet使用spec.volumeClaimTemplates为其pod创建PVC以此绑定PV。回想，之前PVC通过name的方式手动绑定到PV，但，这里肯定不行的，一个StatefulSet管理多个Pod,每个Pod都以自己的PVC,如果通过名字，那它们都会去找同一个PV,一个PV只能有一个PVC绑定，其他PVC就会处于pendding状态；这时，我们再看官方StatefulSets页的demo，会发现有个storageClassName，在Stable Storage下也说明了，StatefulSet创建的PVC是通过storageClassName的方式找到PV的，回到官方部署zk的教程，demo中storageNameClassName是缺失的，即，使用的是默认的StorageClass。
 If no StorageClass is specified, then the default StorageClass will be used
 至此，理解了k8s使用storageClass绑定pvc/pv的设计，也知道在statefulSet中如何使用，但，我们实验要简单，查阅资料storageClass大多是云厂商provisioner,可实验的是NFS和local，当然local更简单，但有个支持版本问题。 &amp;gt; FEATURE STATE: Kubernetes v1.14 实验使用的是1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://scylhy.github.io/k8s/kfk-zk2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://scylhy.github.io/k8s/kfk-zk2/</guid>
      <description>在k8s上搭建kfk-zk集群(2) 这篇接zk的部署方式，整体结构是一样的，包括StorageClass、PV、HeadlessService、StatefulSet，不同的是kfk本身的配置，k8s官方没有k8s的部署教程，我在网上找到一个在k8s上部署kfk的帖子，借用其kfk的模版。
StorageClass我们已经创建，直接使用local-class。
root@hw1:~/zk# kubectl get storageclass NAME PROVISIONER AGE local-class kubernetes.io/no-provisioner 18h  上次仅仅创建了三个PVs,不够用，这里我们为kfk也创建三个PVs，kubectl create -f pv-kfk.yaml。
kind: PersistentVolume apiVersion: v1 metadata: name: datadir-kfk-1 labels: type: local spec: storageClassName: local-class capacity: storage: 5Gi accessModes: - ReadWriteOnce hostPath: path: &amp;quot;/mnt/data11&amp;quot; --- kind: PersistentVolume apiVersion: v1 metadata: name: datadir-kfk-2 labels: type: local spec: storageClassName: local-class capacity: storage: 5Gi accessModes: - ReadWriteOnce hostPath: path: &amp;quot;/mnt/data22&amp;quot; --- kind: PersistentVolume apiVersion: v1 metadata: name: datadir-kfk-3 labels: type: local spec: storageClassName: local-class capacity: storage: 5Gi accessModes: - ReadWriteOnce hostPath: path: &amp;quot;/mnt/data33&amp;quot;  kfk模板借用Kafka on Kubernetes Deploy a highly available Kafka cluster on Kubernetes.</description>
    </item>
    
  </channel>
</rss>